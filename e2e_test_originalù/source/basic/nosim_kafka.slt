# Start with nosim to avoid running in deterministic test


# If we cannot extract key schema, use message key as varchar primary key
statement ok
CREATE TABLE upsert_avro_json_default_key ( primary key (rw_key) )
INCLUDE KEY AS rw_key
WITH (
	connector = 'kafka',
  properties.bootstrap.server = 'message_queue:29092',
	topic = 'upsert_avro_json')
FORMAT UPSERT ENCODE AVRO (schema.registry = 'http://message_queue:8081');

statement ok
CREATE TABLE upsert_student_avro_json ( primary key (rw_key) )
INCLUDE KEY AS rw_key
WITH (
	connector = 'kafka',
  properties.bootstrap.server = 'message_queue:29092',
	topic = 'upsert_student_avro_json')
FORMAT UPSERT ENCODE AVRO (schema.registry = 'http://message_queue:8081');


# TODO: Uncomment this when we add test data kafka key with format `"ID":id`
# statement ok
# CREATE TABLE upsert_avro_json (
#   PRIMARY KEY("ID")
# )
# WITH (
# connector = 'kafka',
#  properties.bootstrap.server = 'message_queue:29092',
# topic = 'upsert_avro_json')
# FORMAT UPSERT ENCODE AVRO (schema.registry = 'http://message_queue:8081');


statement ok
CREATE TABLE debezium_non_compact (PRIMARY KEY(order_id)) with (
    connector = 'kafka',
    kafka.topic = 'debezium_non_compact_avro_json',
    kafka.brokers = 'message_queue:29092',
    kafka.scan.startup.mode = 'earliest'
) FORMAT DEBEZIUM ENCODE AVRO (schema.registry = 'http://message_queue:8081');


statement ok
CREATE TABLE debezium_compact (PRIMARY KEY(order_id)) with (
    connector = 'kafka',
    kafka.topic = 'debezium_compact_avro_json',
    kafka.brokers = 'message_queue:29092',
    kafka.scan.startup.mode = 'earliest'
) FORMAT DEBEZIUM ENCODE AVRO (schema.registry = 'http://message_queue:8081');

statement ok
CREATE TABLE kafka_json_schema_plain with (
    connector = 'kafka',
    kafka.topic = 'kafka_json_schema',
    kafka.brokers = 'kafka:9093',
    kafka.scan.startup.mode = 'earliest'
) FORMAT PLAIN ENCODE JSON (schema.registry = 'http://schemaregistry:8082');

statement ok
CREATE TABLE kafka_json_schema_upsert (PRIMARY KEY(rw_key))
INCLUDE KEY AS rw_key
with (
    connector = 'kafka',
    kafka.topic = 'kafka_upsert_json_schema',
    kafka.brokers = 'kafka:9093',
    kafka.scan.startup.mode = 'earliest'
) FORMAT UPSERT ENCODE JSON (schema.registry = 'http://schemaregistry:8082');

statement ok
flush;

# Wait enough time to ensure SourceExecutor consumes all Kafka data.
sleep 8s

query II
SELECT
 op_type, "ID", "CLASS_ID", "ITEM_ID", "ATTR_ID", "ATTR_VALUE", "ORG_ID", "UNIT_INFO", "UPD_TIME", "DEC_VAL"
FROM
 upsert_avro_json_default_key
ORDER BY
 "ID";
----
update id1 -1 6768 6970 value9 7172 info9 2021-05-18T07:59:58.714Z -21474836.47
delete id2 2 7778 7980 value10 8182 info10 2021-05-19T15:22:45.539Z 99999999.99
delete id3 3 7778 7980 value10 8182 info10 2021-05-19T15:22:45.539Z 21474836.47
delete id5 5 7778 7980 value10 8182 info10 2021-05-19T15:22:45.539Z 21474836.49

# query II
# SELECT
#  *
# FROM
#  upsert_avro_json
# ORDER BY
#  "ID";
# ----
# update id1 -1 6768 6970 value9 7172 info9 2021-05-18T07:59:58.714Z
# delete id2 2 7778 7980 value10 8182 info10 2021-05-19T15:22:45.539Z
# delete id3 3 7778 7980 value10 8182 info10 2021-05-19T15:22:45.539Z
# delete id5 5 7778 7980 value10 8182 info10 2021-05-19T15:22:45.539Z

query II
SELECT
 "ID", "firstName", "lastName", "age", "height", "weight"
FROM
 upsert_student_avro_json
ORDER BY
 "ID";
----
1 Ethan Martinez 18 6.1 180
2 Emily Jackson 19 5.4 110
3 Noah Thompson 21 6.3 195
4 Emma Brown 20 5.3 130
5 Michael Williams 22 6.2 190
6 Leah Davis 18 5.7 140
9 Jacob Anderson 20 5.8 155

query I
select count(*) from debezium_non_compact;
----
2

query I
select count(*) from debezium_compact;
----
2

query TFITT
select
  "dimensions", "price", "productId", "productName", "tags"
from kafka_json_schema_plain
----
(9.5,7,12) 12.5 1 An ice sculpture {cold,ice}

query TFITT
select
  "dimensions", "id", "price", "productName", "tags"
from kafka_json_schema_upsert order by id
----
(9.5,7,12) 1 23 An ice sculpture {cold,ice}
(9.5,7,12) 2 12.5 An ice sculpture {cold,ice}

statement ok
DROP TABLE upsert_avro_json_default_key;

# statement ok
# DROP TABLE upsert_avro_json;

statement ok
DROP TABLE upsert_student_avro_json;

statement ok
DROP TABLE debezium_non_compact;

statement ok
DROP TABLE debezium_compact;

statement ok
DROP TABLE kafka_json_schema_plain;

statement ok
DROP TABLE kafka_json_schema_upsert;